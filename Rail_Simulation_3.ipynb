{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This document is proof-of-work for a datascience competition I recently participated in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "# First of all we import the Flatland rail environment\n",
    "from rail_env import RailEnv\n",
    "from observations import TreeObsForRailEnvAug, feat_order, feat_vec_max\n",
    "\n",
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.envs.schedule_generators import sparse_schedule_generator\n",
    "# We also include a renderer because we want to visualize what is going on in the environment\n",
    "from flatland.utils.rendertools import RenderTool, AgentRenderVariant\n",
    "from flatland.envs.malfunction_generators import malfunction_from_params\n",
    "from flatland.envs.agent_utils import RailAgentStatus\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from scipy.ndimage import rotate\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "# from scipy.sparse import coo_matrix\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.signal import lfilter \n",
    "\n",
    "from gcn import GCN, args\n",
    "from multihead_attention import MultiHeadAttention\n",
    "from rl_models import Value\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale(a, m, M):\n",
    "    return (a-m)/(M-m)\n",
    "\n",
    "def rotate_map(maps, dir_):\n",
    "    rot_deg = (dir_-4)*90\n",
    "    if rot_deg == -360: rot_deg = 0\n",
    "    if rot_deg != 0:\n",
    "        maps = rotate(maps, rot_deg)\n",
    "    return maps\n",
    "\n",
    "def get_rotation_dict():\n",
    "    xx, yy = np.meshgrid(np.arange(2*RAD+1), np.arange(2*RAD+1))\n",
    "    idx_mat = yy*(2*RAD+1) + xx\n",
    "    rotation_dict = dict()\n",
    "    for dir_ in range(0, 4):\n",
    "        rotation_dict[dir_] = rotate_map(idx_mat, dir_).ravel()\n",
    "    return rotation_dict\n",
    "\n",
    "def build_feature_matrix(handle):\n",
    "    \n",
    "    feature_matrix = np.zeros(((2*RAD+1)**2, len(feat_order)))\n",
    "\n",
    "    dist_min_to_target_index = feat_order['dist_min_to_target']\n",
    "    inv_dist_min_to_target_index = feat_order['inv_dist_min_to_target']\n",
    "    try:\n",
    "        cy, cx = env.agents[handle].position\n",
    "    except TypeError:\n",
    "        cy, cx = env.agents[handle].initial_position\n",
    "    dist_scalar = env.obs_builder.search_obs[handle].pop('distance_scalar')\n",
    "    dist_min, dist_max = 0, 0\n",
    "    inv_dist_min, inv_dist_max = 0, 0\n",
    "    idxs = []\n",
    "    for node_id, node in env.obs_builder.search_obs[handle].items():\n",
    "        tot_dist, py, px, dir_ = [int(el) for el in node_id.split('_')] # TODO\n",
    "        y, x = py-cy+RAD, px-cx+RAD\n",
    "        if (y > -1) and y < (2*RAD + 1) and (x > -1) and (x < 2*RAD + 1):\n",
    "            idx = y*(2*RAD + 1) + x\n",
    "            idxs.append(idx)\n",
    "            for ch, el in enumerate(node):\n",
    "                if el is not None:\n",
    "                    if ch == dist_min_to_target_index:\n",
    "                        dist_max = max(dist_max, el)\n",
    "                    if ch == inv_dist_min_to_target_index:\n",
    "                        el = (dist_scalar-el)/dist_scalar\n",
    "                        inv_dist_max = max(inv_dist_max, el)\n",
    "                    feature_matrix[idx, ch] = el\n",
    "    feature_matrix[:, dist_min_to_target_index] = scale(feature_matrix[:, dist_min_to_target_index], dist_min, dist_max)\n",
    "    feature_matrix[:, inv_dist_min_to_target_index] = scale(feature_matrix[:, inv_dist_min_to_target_index], inv_dist_min, inv_dist_max)\n",
    "    \n",
    "    # Ensures all rows of feature matrix are scaled absolutely, so row normalization\n",
    "    # doesn't scale relatively smaller features larger in sparser nodes.\n",
    "    feature_matrix[idxs, -1] = feat_vec_max - feature_matrix[idxs, :].sum(axis=1)\n",
    "    return feature_matrix[rotation_dict[env.agents[handle].direction]]\n",
    "\n",
    "def build_adjacency_matrix(handle):\n",
    "\n",
    "    try:\n",
    "        cy, cx = env.agents[handle].position\n",
    "    except TypeError:\n",
    "        cy, cx = env.agents[handle].initial_position\n",
    "\n",
    "    A = np.zeros(((2*RAD+1)**2, (2*RAD+1)**2))\n",
    "\n",
    "    for branch_id, branch in env.obs_builder.search_branches[handle].items():\n",
    "        _, py, px, _ = [int(el) for el in branch_id.split('_')]\n",
    "        y, x = py-cy+RAD, px-cx+RAD\n",
    "        explore_branch = False\n",
    "        if ((y > -1) and y < (2*RAD + 1) and (x > -1) and (x < 2*RAD + 1)):\n",
    "            explore_branch = True\n",
    "            idx = y*(2*RAD+1) + x\n",
    "        if explore_branch:\n",
    "            for sub_branch in branch:\n",
    "                idxs = []\n",
    "                for py, px in sub_branch:\n",
    "                    y, x = py-cy+RAD, px-cx+RAD\n",
    "                    if (y > -1) and y < (2*RAD + 1) and (x > -1) and (x < 2*RAD + 1):\n",
    "                        idx = y*(2*RAD+1) + x\n",
    "                        idxs.append(idx)\n",
    "                    else:\n",
    "                        explore_branch = False\n",
    "                    if explore_branch == False:\n",
    "                        break\n",
    "                A_row = np.zeros((2*RAD+1)**2)\n",
    "                A_row[idxs] = True\n",
    "                A[idxs, :] += A_row\n",
    "                \n",
    "    A = (A>0).astype(float)\n",
    "\n",
    "    return A[rotation_dict[env.agents[handle].direction], rotation_dict[env.agents[handle].direction]]\n",
    "\n",
    "def build_graph():\n",
    "    graph = dict()\n",
    "    for handle in range(env.number_of_agents):\n",
    "        if not env.agents[handle].status in (RailAgentStatus.DONE, RailAgentStatus.DONE_REMOVED):\n",
    "            nodes = build_feature_matrix(handle)\n",
    "            edges = build_adjacency_matrix(handle)\n",
    "            graph[handle] = (nodes, edges)\n",
    "    return graph\n",
    "\n",
    "def build_inputs(graph):\n",
    "    all_features = np.zeros((env.number_of_agents, (2*RAD+1)**2, len(feat_order)))\n",
    "    all_supports = np.zeros((env.number_of_agents, (2*RAD+1)**2, (2*RAD+1)**2))\n",
    "    for handle, (features, adj) in graph.items():\n",
    "        all_features[handle] = preprocess_features(features)\n",
    "        all_supports[handle] = preprocess_adj(adj)\n",
    "    return all_features, all_supports\n",
    "\n",
    "def build_train_tensors(exps, n = None):\n",
    "    if n is None:\n",
    "        n = len(exps)\n",
    "        \n",
    "    p_feats = csr_matrix((n, KSIZE*feat_dim))\n",
    "    p_supps = csr_matrix((n, KSIZE*KSIZE))\n",
    "\n",
    "    s_feats = csr_matrix((n, KSIZE*feat_dim))\n",
    "    s_supps = csr_matrix((n, KSIZE*KSIZE))\n",
    "\n",
    "    for k, exp in enumerate(exps):\n",
    "        \n",
    "        (p_feat, p_supp), _, (s_feat, s_supp), _, _, _, = simulation_replay_buffer[exp]\n",
    "    \n",
    "        p_feats[k] = p_feat.reshape(KSIZE*feat_dim)\n",
    "        p_supps[k] = p_supp.reshape(KSIZE*KSIZE)\n",
    "    \n",
    "        s_feats[k] = s_feat.reshape(KSIZE*feat_dim)\n",
    "        s_supps[k] = s_supp.reshape(KSIZE*KSIZE)\n",
    "    \n",
    "    p_feats = torch.Tensor(p_feats.toarray().reshape(n, KSIZE, feat_dim)).float()\n",
    "    p_supps = torch.Tensor(p_supps.toarray().reshape(n, KSIZE, KSIZE)).float()\n",
    "\n",
    "    s_feats = torch.Tensor(s_feats.toarray().reshape(n, KSIZE, feat_dim)).float()\n",
    "    s_supps = torch.Tensor(s_supps.toarray().reshape(n, KSIZE, KSIZE)).float()\n",
    "    \n",
    "    return ((p_feats, p_supps), (s_feats, s_supps))\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"\n",
    "    Row-normalize feature matrix\n",
    "    \"\"\"\n",
    "    rowsum = features.sum(1) # get sum of each row, [2708, 1]\n",
    "    r_inv = np.power(rowsum, -1).flatten() # 1/rowsum, [2708]\n",
    "    r_inv[np.isinf(r_inv)] = 0. # zero inf data\n",
    "    r_mat_inv = np.diag(r_inv) # sparse diagonal matrix, [2708, 2708]\n",
    "    return r_mat_inv.dot(features) # D^-1:[2708, 2708]@X:[2708, 2708]\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = ((adj + np.eye(adj.shape[0])) > 0).astype('float')\n",
    "    rowsum = np.array(adj.sum(1)) # D\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten() # D^-0.5\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = np.diag(d_inv_sqrt) # D^-0.5\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt) # D^-0.5AD^0.5\n",
    "\n",
    "def get_connected_agents():\n",
    "    cnnx_mat = csr_matrix((env.number_of_agents, env.number_of_agents))\n",
    "    for handle, nghbr_handles in env.obs_builder.neighbors.items():\n",
    "        cnnx_mat[handle, handle] = 1\n",
    "        if not nghbr_handles:\n",
    "            continue\n",
    "        for nghbr_handle in nghbr_handles:\n",
    "            cnnx_mat[handle, nghbr_handle] = 1\n",
    "            cnnx_mat[nghbr_handle, handle] = 1\n",
    "    return connected_components(csgraph=cnnx_mat, directed=False, return_labels=True)[1]\n",
    "\n",
    "def get_disc_return(rewards, discount):\n",
    "    \"\"\"\n",
    "    C[i] = R[i] + discount * C[i+1]\n",
    "    signal.lfilter(b, a, x, axis=-1, zi=None)\n",
    "    a[0]*y[n] = b[0]*x[n] + b[1]*x[n-1] + ... + b[M]*x[n-M]\n",
    "                          - a[1]*y[n-1] - ... - a[N]*y[n-N]\n",
    "    \"\"\"\n",
    "    r = rewards[::-1]\n",
    "    a = [1, -discount]\n",
    "    b = [1]\n",
    "    y = lfilter(b, a, x=r)\n",
    "    return y[-1]    \n",
    "    \n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "def get_actionable_agents():\n",
    "    eligible = np.zeros(env.number_of_agents, dtype = 'int32')\n",
    "    for handle in range(env.number_of_agents):\n",
    "        eligible[handle] = np.isclose(env.agents[handle].speed_data['position_fraction'], 0.0, rtol=1e-03)\n",
    "    return eligible\n",
    "\n",
    "def get_batch_rewards(exp_handles, n = None):\n",
    "    if n is None:\n",
    "        n = len(exp_handles)\n",
    "    rewards = torch.zeros((n))\n",
    "    for k, exp in enumerate(exp_handles):\n",
    "        rewards[k] = simulation_replay_buffer[exp][3]\n",
    "    rewards = rewards.reshape((n, 1, 1)).float().to(device)\n",
    "    return rewards\n",
    "\n",
    "def get_central_tens(tens, n = None):\n",
    "    if n is None:\n",
    "        n = tens.shape[0]\n",
    "    return tens[list(range(n)), [central_idx]*n]\n",
    "\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "def optimal_path_search(idx, digraph, min_dists_vec, rewards, d1idx = None, depth = 0):\n",
    "    \n",
    "    if rewards is None:\n",
    "        rewards = []\n",
    "        \n",
    "    tot_dist = 0\n",
    "    \n",
    "    is_sub_search = depth > 0\n",
    "    if not is_sub_search and not digraph[d1idx].nonzero()[1].any():\n",
    "\n",
    "        rewards.append(0)\n",
    "        d1idx = None\n",
    "    \n",
    "    if is_sub_search:\n",
    "        tot_dist += 1\n",
    "    \n",
    "    action_is_optimal = False\n",
    "    if not is_sub_search:\n",
    "        action_is_optimal = False\n",
    "        \n",
    "    exploring = True\n",
    "    nidx = None\n",
    "    r = np.inf\n",
    "    \n",
    "    while exploring:\n",
    "        nghbrs = list(set(digraph[idx].nonzero()[1]) - {idx})\n",
    "        if not nghbrs:\n",
    "            break\n",
    "        nghbrs_min_dists = min_dists_vec[nghbrs]\n",
    "        ### Correcting for errors thrown when tree search depth exceeded.\n",
    "        try:\n",
    "            nidx = nghbrs[np.argmin(nghbrs_min_dists)]\n",
    "        except ValueError:\n",
    "            break\n",
    "        if nidx == idx:\n",
    "            break\n",
    "        else:\n",
    "            idx = nidx\n",
    "        if not is_sub_search and d1idx is not None:\n",
    "            if d1idx == idx:\n",
    "                action_is_optimal == True\n",
    "        r = min(min(nghbrs_min_dists), r)\n",
    "        tot_dist += 1\n",
    "        \n",
    "    if r == np.inf:\n",
    "        r = 0\n",
    "    \n",
    "    if not is_sub_search:\n",
    "        rewards.insert(0, r**(tot_dist + eps))\n",
    "\n",
    "    elif is_sub_search and d1idx is not None:\n",
    "        rewards.append(r**(tot_dist + eps))\n",
    "    \n",
    "    if d1idx is not None and not is_sub_search:\n",
    "        if action_is_optimal == True:\n",
    "            rewards.append(r**(tot_dist - 1 + eps))\n",
    "            \n",
    "        elif action_is_optimal == False:\n",
    "            rewards = optimal_path_search(d1idx, digraph, min_dists_vec, rewards, d1idx, depth = 1)\n",
    "            \n",
    "    return rewards\n",
    "\n",
    "from enum import IntEnum\n",
    "\n",
    "class RailEnvActions(IntEnum):\n",
    "    DO_NOTHING = 0  # implies change of direction in a dead-end!\n",
    "    MOVE_LEFT = 1\n",
    "    MOVE_FORWARD = 2\n",
    "    MOVE_RIGHT = 3\n",
    "    STOP_MOVING = 4\n",
    "\n",
    "    @staticmethod\n",
    "    def to_char(a: int):\n",
    "        return {\n",
    "            0: 'B',\n",
    "            1: 'L',\n",
    "            2: 'F',\n",
    "            3: 'R',\n",
    "            4: 'S',\n",
    "        }[a]\n",
    "\n",
    "\n",
    "def my_controller():\n",
    "    \"\"\"\n",
    "    You are supposed to write this controller\n",
    "    \"\"\"\n",
    "    _action = {}\n",
    "    for _idx in range(env.number_of_agents):\n",
    "        _action[_idx] = RailEnvActions(np.random.randint(0, 5))\n",
    "    return _action\n",
    "    \n",
    "IntActions = {\n",
    "    RailEnvActions.DO_NOTHING:0,\n",
    "    RailEnvActions.MOVE_LEFT:1,\n",
    "    RailEnvActions.MOVE_FORWARD:2,\n",
    "    RailEnvActions.MOVE_RIGHT:3,\n",
    "    RailEnvActions.STOP_MOVING:4\n",
    "}\n",
    "\n",
    "def get_reward(handle, actions, graph):\n",
    "    \n",
    "    penalty = 0\n",
    "    \n",
    "    action = actions[handle]\n",
    "    \n",
    "    min_dists_vec = graph[handle][0][:, feat_order['dist_min_to_target']]\n",
    "    digraph = env.obs_builder.digraphs[handle]\n",
    "    cy, cx = RAD, RAD\n",
    "\n",
    "    if action == 0:\n",
    "        ncy, ncx = cy, cx\n",
    "        if not env.agents[handle].moving:\n",
    "            penalty = .2\n",
    "    \n",
    "    elif action == 1:\n",
    "        ncy, ncx = cy, cx-1\n",
    "    \n",
    "    elif action == 2:\n",
    "        ncy, ncx = cy-1, cx\n",
    "    \n",
    "    elif action == 3:\n",
    "        ncy, ncx = cy, cx+1\n",
    "    \n",
    "    elif action == 4:\n",
    "        ncy, ncx = cy, cx\n",
    "        penalty = .2\n",
    "    \n",
    "    n_central_idx = ncy*(2*RAD+1) + ncx\n",
    "    \n",
    "    outs = optimal_path_search(central_idx, digraph, min_dists_vec, None, n_central_idx, 0)\n",
    "    reward = -abs(outs[0]-outs[1]-penalty)\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 100  # With of map\n",
    "height = 100  # Height of map\n",
    "nr_trains = 50  # Number of trains that have an assigned task in the env\n",
    "cities_in_map = 20  # Number of cities where agents can start or end\n",
    "seed = 25  # Random seed\n",
    "grid_distribution_of_cities = False  # Type of city distribution, if False cities are randomly placed\n",
    "max_rails_between_cities = 2  # Max number of tracks allowed between cities. This is number of entry point to a city\n",
    "max_rail_in_cities = 6  # Max number of parallel tracks within a city, representing a realistic trainstation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rail_generator = sparse_rail_generator(max_num_cities=cities_in_map,\n",
    "                                       seed=seed,\n",
    "                                       grid_mode=grid_distribution_of_cities,\n",
    "                                       max_rails_between_cities=max_rails_between_cities,\n",
    "                                       max_rails_in_city=max_rail_in_cities,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The schedule generator can make very basic schedules with a start point, end point and a speed profile for each agent.\n",
    "# The speed profiles can be adjusted directly as well as shown later on. We start by introducing a statistical\n",
    "# distribution of speed profiles\n",
    "\n",
    "# Different agent types (trains) with different speeds.\n",
    "speed_ration_map = {1.: 0.25,  # Fast passenger train\n",
    "                    1. / 2.: 0.25,  # Fast freight train\n",
    "                    1. / 3.: 0.25,  # Slow commuter train\n",
    "                    1. / 4.: 0.25}  # Slow freight train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now initiate the schedule generator with the given speed profiles\n",
    "schedule_generator = sparse_schedule_generator(speed_ration_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can furthermore pass stochastic data to the RailEnv constructor which will allow for stochastic malfunctions\n",
    "# during an episode.\n",
    "\n",
    "stochastic_data = {'prop_malfunction': 0.3,  # Percentage of defective agents\n",
    "                   'malfunction_rate': 30,  # Rate of malfunction occurence\n",
    "                   'min_duration': 3,  # Minimal duration of malfunction\n",
    "                   'max_duration': 20  # Max duration of malfunction\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree predictor\n",
    "max_depth = 3\n",
    "RAD = 5\n",
    "KSIZE = (2*RAD+1)**2\n",
    "central_idx = (2*RAD+1)*RAD + RAD\n",
    "feat_dim = len(feat_order)\n",
    "observation_builder = TreeObsForRailEnvAug(max_depth=max_depth, RAD=RAD)\n",
    "rotation_dict = get_rotation_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom observation builder with predictor, uncomment line below if you want to try this one\n",
    "# observation_builder = TreeObsForRailEnv(max_depth=2, predictor=ShortestPathPredictorForRailEnv())\n",
    "\n",
    "# Construct the enviornment with the given observation, generataors, predictors, and stochastic data\n",
    "env = RailEnv(width=width,\n",
    "              height=height,\n",
    "              rail_generator=rail_generator,\n",
    "              schedule_generator=schedule_generator,\n",
    "              number_of_agents=nr_trains,\n",
    "              malfunction_generator_and_process_data=malfunction_from_params(stochastic_data),\n",
    "              obs_builder_object=observation_builder,\n",
    "              remove_agents_at_target=True  # Removes agents at the end of their journey to make space for others\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim: 26\n",
      "output dim: 128\n",
      "num_features_nonzero: 26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (layer1): GraphConvolution(\n",
       "    (affine1): Linear(in_features=26, out_features=128, bias=False)\n",
       "  )\n",
       "  (layer2): GraphConvolution(\n",
       "    (affine1): Linear(in_features=128, out_features=128, bias=False)\n",
       "  )\n",
       "  (layer3): GraphConvolution(\n",
       "    (affine1): Linear(in_features=128, out_features=128, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn = GCN(feat_dim, args['hidden'], feat_dim)\n",
    "gcn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  in_features=128, head_num=8, bias=True, activation=<function relu at 0x000001AEF7131708>\n",
       "  (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_num = 8\n",
    "attention_layer = MultiHeadAttention(args['hidden'], head_num)\n",
    "attention_layer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(\n",
       "  (affine1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (affine2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (affine3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (value_head): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_net = Value(2*args['hidden'])\n",
    "value_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36096, 16512, 43265)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_params(gcn), get_n_params(attention_layer), get_n_params(value_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "270.791684 second runtime\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "gamma = .99\n",
    "\n",
    "simulation_num = 0\n",
    "simulation_replay_buffer = dict()\n",
    "simulation_rewards = dict([(handle, []) for handle in range(env.number_of_agents)])\n",
    "simulation_saved_experiences = set()\n",
    "experience_backlog = set()\n",
    "\n",
    "_, _ = env.reset()\n",
    "actionable_agents = get_actionable_agents()\n",
    "p_graph = build_graph()\n",
    "p_features, p_supports = build_inputs(p_graph)\n",
    "agent_groups = get_connected_agents()\n",
    "interesting_experiences = np.random.randint(2, size=env.number_of_agents)\n",
    "interesting_experiences *= actionable_agents\n",
    "\n",
    "max_timesteps = 500\n",
    "for timestep in range(max_timesteps):\n",
    "    \n",
    "    print(timestep)\n",
    "        \n",
    "    actions = my_controller()\n",
    "    obs, all_rewards, done, info = env.step(actions)\n",
    "    actions = dict([(handle, IntActions[env.agents[handle].speed_data['transition_action_on_cellexit']]) for handle in range(env.number_of_agents)])\n",
    "    s_graph = build_graph()\n",
    "    s_features, s_supports = build_inputs(s_graph)\n",
    "\n",
    "    for handle in range(env.number_of_agents):\n",
    "        simulation_rewards[handle].append(all_rewards[handle])\n",
    "        if env.agents[handle] is not RailAgentStatus.DONE_REMOVED and interesting_experiences[handle] == 1:\n",
    "            experience_id = '{}_{}_{}'.format(simulation_num, timestep, handle)\n",
    "            p_state = (csr_matrix(p_features[handle]), csr_matrix(p_supports[handle]))\n",
    "            action = deepcopy(actions[handle])\n",
    "            s_state = (csr_matrix(s_features[handle]), csr_matrix(s_supports[handle]))\n",
    "            group_handles = list(set(np.where(agent_groups == agent_groups[handle])[0]) - {handle})\n",
    "            for othr_handle in group_handles:\n",
    "                if interesting_experiences[othr_handle] != 1:\n",
    "                    if othr_handle < handle:\n",
    "                        experience_backlog |= {othr_handle}\n",
    "                    elif othr_handle > handle:\n",
    "                        interesting_experiences[othr_handle] = 1\n",
    "            nghbrs = env.obs_builder.neighbors[handle]\n",
    "            nghbr_experience_ids = None\n",
    "            if nghbrs:\n",
    "                nghbr_experience_ids = list(set(['{}_{}_{}'.format(simulation_num, timestep, nghbr) for nghbr in nghbrs]))\n",
    "            simulation_saved_experiences |= {experience_id}\n",
    "            agent_group = ['{}_{}_{}'.format(simulation_num, timestep, ag) for ag in group_handles]\n",
    "            if not agent_group:\n",
    "                agent_group = None\n",
    "            reward = get_reward(handle, actions, p_graph)\n",
    "            simulation_replay_buffer[experience_id] = [p_state, action, s_state, reward, nghbr_experience_ids, agent_group]\n",
    "        \n",
    "    for handle in experience_backlog:\n",
    "        experience_id = '{}_{}_{}'.format(simulation_num, timestep, handle)\n",
    "        p_state = (csr_matrix(p_features[handle]), csr_matrix(p_supports[handle]))\n",
    "        action = deepcopy(actions[handle])\n",
    "        s_state = (csr_matrix(s_features[handle]), csr_matrix(s_supports[handle]))\n",
    "        group_handles = list(set(np.where(agent_groups == agent_groups[handle])[0]) - {handle})\n",
    "        nghbrs = env.obs_builder.neighbors[handle]\n",
    "        nghbr_experience_ids = None\n",
    "        if nghbrs:\n",
    "            nghbr_experience_ids = list(set(['{}_{}_{}'.format(simulation_num, timestep, nghbr) for nghbr in nghbrs]))\n",
    "        simulation_saved_experiences |= {experience_id}\n",
    "        agent_group = ['{}_{}_{}'.format(simulation_num, timestep, ag) for ag in group_handles]\n",
    "        if not agent_group:\n",
    "            agent_group = None\n",
    "        reward = get_reward(handle, actions, p_graph)\n",
    "        simulation_replay_buffer[experience_id] = [p_state, action, s_state, reward, nghbr_experience_ids, agent_group]\n",
    "        \n",
    "    actionable_agents = get_actionable_agents()\n",
    "    p_features, p_supports = deepcopy(s_features), deepcopy(s_supports)\n",
    "    agent_groups = get_connected_agents()\n",
    "    interesting_experiences = np.random.randint(2, size=env.number_of_agents)\n",
    "    interesting_experiences *= actionable_agents\n",
    "    experience_backlog = set()\n",
    "    \n",
    "end = time.time()\n",
    "print('{} second runtime'.format(round(end-start, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1aebcb9f608>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "handle = np.random.randint(env.number_of_agents)\n",
    "plt.imshow(s_graph[handle][0][:, feat_order['dist_min_to_target']].reshape((2*RAD+1, 2*RAD+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1aeb9b8ae48>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAALH0lEQVR4nO3dbYild3nH8e9vZ3cTNw81wVbqbmiSGmKDpUQGiQakJAqxiim0hQQiNhT2TdUYQkPsm7wtVERfSGCJsYIhQdaUhhB8ICpFsFsnD2A2q5hGm4yJTVpt1FB3N+zVF3NsJ9Ndsz33febc7fX9QJg5D/mfi918c5+He/6TqkLS/387lj2ApO1h7FITxi41YexSE8YuNbFzOx/s7PN21/l7zxxtvRVOjLYWwL8d3j3qelO273d/Pup6//ST14+63hnrL426Xhe/4CWO1dGc7LZtjf38vWfyFwdXR1vvnB3/MdpaAHe/ad+o603ZXz/wD6Ou90efv3nU9S6+9ZujrtfFoXrolLf5NF5qwtilJoxdasLYpSaMXWpiUOxJrkny3SRPJrltrKEkjW/u2JOsAJ8C3g1cBlyf5LKxBpM0riFH9rcCT1bVU1V1DLgXuHacsSSNbUjse4FnNl1en133Ckn2J1lLsvbznxwb8HCShhgS+8lOyfsfO2FU1YGqWq2q1bPP63M6qjQ1Q2JfBy7YdHkf8OywcSQtypDYvwVckuSiJLuB64D7xxlL0tjm/kGYqno5yQeBLwErwF1VdXi0ySSNatBPvVXVg8CDI80iaYE8g05qwtilJoxdasLYpSa2dVuqFU6MupXUuSu/GG0tgFueHO/DhHN3jDvbnh3HR13vnLw86nondvmbhabOI7vUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUxPbuQZcTo+4btydHR1sLxt03bup7xp2142S/l3N+tdM96KbOI7vUhLFLTRi71ISxS00Yu9SEsUtNzB17kguSfC3JkSSHk9w05mCSxjXkc/aXgVuq6pEk5wAPJ/lKVT0x0mySRjT3kb2qnquqR2bf/ww4AuwdazBJ4xrlNXuSC4HLgUMnuW1/krUkay/+eNyzwCSdvsGxJzkb+ALwkar66dbbq+pAVa1W1eqvnb+tZ+dK2mRQ7El2sRH63VV13zgjSVqEIe/GB/g0cKSqPj7eSJIWYciR/Urg/cBVSR6b/fMHI80laWRzv4iuqm8A4/6cpKSF8Qw6qQljl5owdqmJbf3g+/nHz+SOS964nQ+pU/jb9X8cdb3a5bZUU+eRXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrCX77W1J4du8ddcMU96KbOI7vUhLFLTRi71ISxS00Yu9SEsUtNDI49yUqSR5M8MMZAkhZjjCP7TcCREdaRtECDYk+yD3gPcOc440halKFH9k8AtwInTnWHJPuTrCVZO87RgQ8naV5zx57kvcDzVfXwr7pfVR2oqtWqWt3FGfM+nKSBhhzZrwTel+QHwL3AVUk+N8pUkkY3d+xV9dGq2ldVFwLXAV+tqhtGm0zSqPycXWpilB9xraqvA18fYy1Ji+GRXWrC2KUmjF1qwtilJtyDrqnf/uqNo66XXac8iVIT4ZFdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasI96Jp64w2PLnsEbTOP7FITxi41YexSE8YuNWHsUhPGLjUxKPYkr01yMMl3khxJ8raxBpM0rqGfs38S+GJV/XGS3cCeEWaStABzx57kXOAdwJ8CVNUx4Ng4Y0ka25Cn8RcDLwCfSfJokjuTnLX1Tkn2J1lLsnacowMeTtIQQ2LfCbwFuKOqLgdeAm7beqeqOlBVq1W1uoszBjycpCGGxL4OrFfVodnlg2zEL2mC5o69qn4EPJPk0tlVVwNPjDKVpNENfTf+Q8Dds3finwJuHD6SpEUYFHtVPQasjjSLpAXyDDqpCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaGBR7kpuTHE7yeJJ7kpw51mCSxjV37En2Ah8GVqvqzcAKcN1Yg0ka19Cn8TuB1yTZCewBnh0+kqRFmDv2qvoh8DHgaeA54MWq+vLW+yXZn2Qtydpxjs4/qaRBhjyNPw+4FrgIeANwVpIbtt6vqg5U1WpVre7ijPknlTTIkKfx7wS+X1UvVNVx4D7g7eOMJWlsQ2J/GrgiyZ4kAa4GjowzlqSxDXnNfgg4CDwCfHu21oGR5pI0sp1D/uWquh24faRZJC2QZ9BJTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTbxq7EnuSvJ8ksc3XXd+kq8k+d7s63mLHVPSUKdzZP8b4Jot190GPFRVlwAPzS5LmrBXjb2q/h748ZarrwU+O/v+s8AfjjyXpJHN+5r99VX1HMDs62+c6o5J9idZS7J2nKNzPpykoRb+Bl1VHaiq1apa3cUZi344Sacwb+z/kuQ3AWZfnx9vJEmLMG/s9wMfmH3/AeDvxhlH0qKczkdv9wDfBC5Nsp7kz4C/At6V5HvAu2aXJU3Yzle7Q1Vdf4qbrh55FkkL5Bl0UhPGLjVh7FITxi41karavgdLXgD++TTu+jrgXxc8zrymPBtMe74pzwbTnu90Z/utqvr1k92wrbGfriRrVbW67DlOZsqzwbTnm/JsMO35xpjNp/FSE8YuNTHV2A8se4BfYcqzwbTnm/JsMO35Bs82ydfsksY31SO7pJEZu9TEpGJPck2S7yZ5Msmk9rVLckGSryU5kuRwkpuWPdNWSVaSPJrkgWXPslWS1yY5mOQ7sz/Dty17pl9KcvPs7/TxJPckOXPJ8yxkk9fJxJ5kBfgU8G7gMuD6JJctd6pXeBm4pap+B7gC+POJzQdwE3Bk2UOcwieBL1bVm4DfYyJzJtkLfBhYrao3AyvAdcudajGbvE4mduCtwJNV9VRVHQPuZWNjy0moqueq6pHZ9z9j4z/Wvcud6r8l2Qe8B7hz2bNsleRc4B3ApwGq6lhV/ftyp3qFncBrkuwE9gDPLnOYRW3yOqXY9wLPbLq8zoRi2izJhcDlwKHlTvIKnwBuBU4se5CTuBh4AfjM7GXGnUnOWvZQAFX1Q+BjwNPAc8CLVfXl5U51Uqe9yeupTCn2nOS6yX0umORs4AvAR6rqp8ueByDJe4Hnq+rhZc9yCjuBtwB3VNXlwEtM5HcNzF77XgtcBLwBOCvJDcudajGmFPs6cMGmy/tY8tOprZLsYiP0u6vqvmXPs8mVwPuS/ICNlz9XJfncckd6hXVgvap++UzoIBvxT8E7ge9X1QtVdRy4D3j7kmc6mcGbvE4p9m8BlyS5KMluNt4kuX/JM/2XJGHjNeeRqvr4sufZrKo+WlX7qupCNv7cvlpVkzk6VdWPgGeSXDq76mrgiSWOtNnTwBVJ9sz+jq9mIm8ebjF4k9dX3YNuu1TVy0k+CHyJjXdE76qqw0sea7MrgfcD307y2Oy6v6yqB5c40/8lHwLunv2P/CngxiXPA0BVHUpyEHiEjU9cHmXJp83ONnn9feB1SdaB29nY1PXzsw1fnwb+5H+9rqfLSj1M6Wm8pAUydqkJY5eaMHapCWOXmjB2qQljl5r4T1CgjWtPQA2KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "handle = np.random.randint(env.number_of_agents)\n",
    "plt.imshow(s_graph[handle][0][:, feat_order['dist_min_to_target']].reshape((2*RAD+1, 2*RAD+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1aebce56048>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "handle = np.random.randint(env.number_of_agents)\n",
    "plt.imshow(s_graph[handle][0][:, feat_order['dist_min_to_target']].reshape((2*RAD+1, 2*RAD+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5254"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simulation_replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(list(gcn.parameters()) + list(attention_layer.parameters()) + list(value_net.parameters()), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Training Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431.564655 second runtime\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for itrn in range(1000):\n",
    "    \n",
    "    # sampling batch of experiences\n",
    "    batch_size = 32\n",
    "    batch_exp = np.random.choice(list(simulation_replay_buffer.keys()), batch_size)\n",
    "    for exp in batch_exp:\n",
    "        simulation_replay_buffer[exp]\n",
    "    try:\n",
    "        # batches\n",
    "        batch_nghbrs = []\n",
    "        batch_nghbrs_dc = dict()\n",
    "        for exp in batch_exp:\n",
    "            nghbrs = simulation_replay_buffer[exp][-2]\n",
    "            batch_nghbrs_dc[exp] = nghbrs\n",
    "            if nghbrs is not None:\n",
    "                batch_nghbrs += nghbrs\n",
    "        for nghbr_exp in batch_nghbrs:\n",
    "            simulation_replay_buffer[nghbr_exp]\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    # groups\n",
    "    batch_groups = []\n",
    "    batch_groups_dc = dict()\n",
    "    for exp in batch_exp:\n",
    "        group = simulation_replay_buffer[exp][-1]\n",
    "        batch_groups_dc[exp] = group\n",
    "        if group is not None:\n",
    "            batch_groups += group\n",
    "    for group_exp in batch_groups:\n",
    "        simulation_replay_buffer[group_exp]\n",
    "\n",
    "    # Build Neighbor tensors and Pass neighbors through GCN without autograd\n",
    "\n",
    "    if batch_nghbrs:\n",
    "        (nghbr_p_feats, nghbr_p_supps), (nghbr_s_feats, nghbr_s_supps) = build_train_tensors(batch_nghbrs)\n",
    "        nghbr_p_feats, nghbr_p_supps = nghbr_p_feats.to(device), nghbr_p_supps.to(device)\n",
    "    \n",
    "        gcn.eval()\n",
    "        with torch.no_grad():\n",
    "            nghbr_p_gcn_outs = gcn((nghbr_p_feats, nghbr_p_supps))\n",
    "            \n",
    "        nghbr_p_gcn_central_outs = get_central_tens(nghbr_p_gcn_outs[0], len(batch_nghbrs))\n",
    "    \n",
    "    # Build Agent tensors and Pass agents through GCN with autograd\n",
    "    (p_feats, p_supps), (s_feats, s_supps) = build_train_tensors(batch_exp, batch_size)\n",
    "    p_feats, p_supps = p_feats.to(device), p_supps.to(device)\n",
    "\n",
    "    gcn.train()\n",
    "    p_gcn_outs = gcn((p_feats, p_supps))\n",
    "\n",
    "    p_gcn_central_outs = get_central_tens(p_gcn_outs[0], batch_size)\n",
    "\n",
    "    # Pass through Attention Layer and Value Network\n",
    "    v_net_input = torch.zeros((batch_size, 1, 2*args['hidden'])).float().to(device)\n",
    "    for k, exp in enumerate(batch_exp):\n",
    "        nghbrs_idx = None\n",
    "        is_dummy = True\n",
    "        nghbrs = batch_nghbrs_dc[exp]\n",
    "        if nghbrs is not None:\n",
    "            nghbrs_idx = [batch_nghbrs.index(nghbr) for nghbr in nghbrs]\n",
    "        if nghbrs_idx is not None:\n",
    "            idx_nghbr_p_gcn_central_outs = nghbr_p_gcn_central_outs[nghbrs_idx].reshape((1, len(nghbrs_idx), args['hidden']))\n",
    "            is_dummy = False\n",
    "        else:\n",
    "            dummy_tens = torch.zeros((1, 1, args['hidden'])).float().to(device).detach()\n",
    "\n",
    "        if not is_dummy:\n",
    "            idx_p_att_central_outs = attention_layer(p_gcn_central_outs[k].reshape((1, 1, args['hidden'])),\n",
    "                                                     idx_nghbr_p_gcn_central_outs, \n",
    "                                                     idx_nghbr_p_gcn_central_outs)\n",
    "    \n",
    "            idx_p_conc_central_outs = torch.cat((p_gcn_central_outs[k].reshape((1, 1, args['hidden'])), \n",
    "                                                 idx_p_att_central_outs), -1)\n",
    "\n",
    "        else:\n",
    "            idx_p_conc_central_outs = torch.cat((p_gcn_central_outs[k].reshape((1, 1, args['hidden'])), dummy_tens), -1)\n",
    "    \n",
    "        v_net_input[k] = idx_p_conc_central_outs.reshape((1, 2*args['hidden']))\n",
    "    \n",
    "    value_net.train()\n",
    "    value_tens = value_net(v_net_input)\n",
    "\n",
    "    batch_rewards = get_batch_rewards(batch_exp, batch_size)\n",
    "\n",
    "    loss = (((value_tens-batch_rewards)**2).sum())/2\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "end = time.time()\n",
    "print('{} second runtime'.format(round(end-start, 6)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10331648, 75497472)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(), torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly sampled batch rewards are too sparse, sample \"interesting\" experiences only, defined by agent proximity to other agents or multi-directional switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
